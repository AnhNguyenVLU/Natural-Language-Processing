# Assigment 08 - Improved Language Modeling

Use my code as much as possible.

Recall the language model you did - Harry Potter, Jokes, Python, etc.

1.   Change LSTM to Transformer (note that you only need the decoder part, NOT the encoder).     How about encoder outputs because we don't have - if you check GPT1, 2, 3, they simply cut the cross_attention component.

2.   Compare greedy decoding + beam search coding in inference.   Note that in my beam search code, I put "pass" in my greedy decoding.  Please fill it in.

3.   Deploy this new method to your old website.

![image](https://user-images.githubusercontent.com/74288640/224563151-aad484ea-df68-4f3f-bcd4-cdb8ff93e439.png)






