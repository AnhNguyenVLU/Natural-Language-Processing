# Natural-Language-Processing

## READING PAPER NLP

### Paper 1: Analyzing Wrap-Up Effects through an Information-Theoretic Lens
Link paper: https://aclanthology.org/2022.acl-short.3 \
Date: 31/01/2023
| Topic        |                 Analyzing Wrap-Up Effects through an Information-Theoretic Lens                                             |
|--------------|--------------------------------------------------------------------------------------------------------|
| Question Reserch    | - Data measured on words at the end of a sentence is often omitted. Consequently, the understanding of the cognitive processes that might be involved in these wrap-up effects is limited. <br /> - Are word and context surprisals that matter in real time (RT) data analysis? |
| Related Work | - Studies analyzing reading times have been employed to explore a number of psycholinguistic theories.<br /> - The existence of wrap-up effects is well-known the cognitive processes giving rise to them are still not fully understood.<br /> - Most studies of online processing omit data from these words to explicitly control for the confounding factors wrap-up effects introduce.<br /> - The few studies on wrap-up effects rely on smalldatasets, none of which analyze naturalistic text.<br /> - The long line of work that has connected information-theoretic measures and psychometric data employing similar methods to build models of sentence- and clause-final RTs. |
| Solution     | The author attempts to learn more about these processes by examining the relationship between enveloping effects and information theory quantities, such as word and context surprisals. |
| Method       | - Exploring the relationship between clause-final RTs and information-theoretic attributes of text.<br /> - Use surprise estimates from modern language models to look for associations between ending effects and informational content in a sentence.<br /> - Wrap-up effects.<br /> - Information-theoretic|
| Result       | - The author find that operationalizations of the information contained in preceding context lead to better predictions of these RTs, while not adding significant predictive power for sentence-medial RTs.<br /> - Provide evidence (either in support or against) about several theories of the nature of wrap-up processes. |

### Paper 2: On the probability–quality paradox in language generation
Link paper: https://aclanthology.org/2022.acl-short.5 \
Date: 13/02/2023
| Topic        |                 On the probability–quality paradox in language generation                                            |
|--------------|--------------------------------------------------------------------------------------------------------|
| Question Reserch    | Why is the lower-probability text generated by stochastic methods is perceived as more human-like? |
| Related Work |  - In the domain of machine translation, the most probable string under the model is often the empty string. <br /> - In the domain of openended generation, mode-seeking methods produce dull and generic text. <br /> - The set of stringshas an intuitive relationship to the typical set, an information theoretic concept defined for stationary ergodic stochastic processes. |
| Solution     | - The author offer an explanation by analyzing language generation through an information-theoretic lens. <br /> - Provide preliminary empirical evidence in favor of this hypothesis. <br /> - Quality ratings of both human and machine-generated text-covering multiple tasks and common decoding strategies  |
| Method       | - An analysis comparing human and model-generated text, investigating multiple common decoding strategies and NLG tasks.<br /> - Analysis focuses exclusively on English text. <br /> -The author take these observations as empirical support for hypothesis, helping to explain the probability–quality.|
| Result       | - Quality text has an information content significantly closer to the entropy than expect by chance. <br /> - The author provide empirical evidence in support of our hypothesis in an analysis of both human and machine-generated text, demonstrating that, overwhelmingly, high-quality text indeed has information content in the proposed region. |

### Paper 3: Type-Driven Multi-Turn Corrections for Grammatical Error Correction
Link paper: https://aclanthology.org/2022.findings-acl.254 \
Date: 25/02/2023
| Topic        |          Type-Driven Multi-Turn Corrections for Grammatical Error Correction          |
|--------------|--------------------------------------------------------------------------------------------------------|
| Question Reserch   | In what way is the author's model trained to not only fix errors incrementally, but also exploit interdependencies between different types of errors for better performance?|
| Related Work |  - There are two categories of models in GEC: Transformer-dominant NMT-based models and GECToR-leading Seq2Label models <br /> - By comparison, Seq2Label models are able to correct grammatical errors more efficiently and even better. <br /> - Since GEC models may fail to completely correct a sentence through just one-iteration inference, some researchers resort to data augmentation that has been widely used in other NLP studies |
| Solution     | Using this TypeDriven Multi-Turn Corrections, from each training instance, author additionally construct multiple training instances, each of which involves the correction of a specific type of errors. Then, author use these additionally-constructed training instances and the original one to train the model in turn. |
| Method       | The authors introduce a model that uses a type-driven architecture, which means that it uses the syntactic and semantic information of the sentence to identify the errors and generate the corrections. The model is also capable of taking into account multiple turns of corrections, which can improve the accuracy of the correction process. |
| Result       | A TypeDriven Multi-Turn Corrections approach for GEC show that result improving the accuracy of GEC, particularly on sentences with multiple errors. |

### Paper 4: AUTOPROMPT: Eliciting Knowledge from Language Models with Automatically Generated Prompts
Link paper: https://aclanthology.org/2020.emnlp-main.346.pdf \
Date: 06/03/2023
| Topic        |                 AUTOPROMPT: Eliciting Knowledge from Language Models with Automatically Generated Prompts                                           |
|--------------|--------------------------------------------------------------------------------------------------------|
| Question Reserch    | How to generate prompts automatically to elicit knowledge from language models and use the generated knowledge to improve downstream tasks |
| Related Work |  - Prompt Engineering: The paper builds upon the concept of prompt engineering, which involves designing high-quality prompts to improve the performance of language models on specific tasks. Prompt engineering has been used in various natural language processing applications, such as question answering, text classification, and sentiment analysis. <br /> - Few-Shot and Zero-Shot Learning: The paper also relates to recent work on few-shot and zero-shot learning, which aims to train models on a small number of examples or no examples at all, respectively. This line of research has gained significant attention due to its potential to reduce the need for large amounts of annotated data. <br /> - Fine-Tuning of Language Models: The paper also relates to previous work on fine-tuning language models, which involves re-training a pre-trained language model on a specific task using a small amount of task-specific data. Fine-tuning has been shown to be an effective approach for improving the performance of language models on a wide range of tasks, including text classification, language generation, and question answering.|
| Solution     | The paper proposes a method called AUTOPROMPT that automatically generates prompts to elicit knowledge from language models. AUTOPROMPT uses a two-stage process:  <br />  - It generates a set of prompts using an iterative algorithm that maximizes the expected quality of the generated prompts. <br /> - It fine-tunes the language model on the generated prompts and uses the resulting model to improve downstream tasks.  |
| Method       | The method involves several steps: <br /> 1. The algorithm generates a set of candidate prompts by sampling from a set of template prompts. <br /> 2. Each candidate prompt is scored by the language model based on the likelihood of the prompt given the context. <br /> 3. The algorithm selects the highest scoring prompts and generates new prompts by modifying the selected prompts using different heuristics. <br /> 4. The process continues until a fixed number of prompts are generated. <br /> 5. The language model is fine-tuned on the generated prompts, and the resulting model is used to improve downstream tasks. |
| Result       | The paper demonstrates that AUTOPROMPT outperforms several baseline methods in a range of tasks, including text classification, named entity recognition, and question answering. The experiments show that AUTOPROMPT generates prompts that are more effective in eliciting knowledge from the language model and improving downstream tasks. The paper also shows that AUTOPROMPT can be used to perform zero-shot and few-shot learning, where the model is trained on a small number of examples. |

